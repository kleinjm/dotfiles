# Optimize Tests Command

## Purpose
**DIRECTLY MODIFY** existing test files in the current branch to improve performance while maintaining coverage and confidence. Focus on combining tests, reducing setup overhead, and following the "fewer tests with more expectations" philosophy.

## Usage
Run this command at the end of a development session to optimize all test changes in place.

```bash
# To see what test files have changed
git diff --name-only main...HEAD | grep -E "spec/.*_spec\.rb$"
```

## Prerequisites
Before running optimizations, ensure all current work is committed:

1. Check git status for any staged or unstaged changes
2. If changes exist, create a commit with all current work using an appropriate commit message
3. Confirm the working directory is clean before proceeding with test optimizations

This ensures that test optimization changes can be easily reviewed and reverted separately from feature work.

## Instructions for Claude

### CRITICAL: Do NOT Create New Files
- **DO NOT** create any new spec files with "_optimized" suffixes
- **DO NOT** create summary documents or reports
- **DIRECTLY EDIT** the existing spec files in place using the Edit tool
- **REPLACE** the original test content with optimized versions

### Overview
Analyze all test files modified in the current branch (diff against main) and optimize them IN PLACE for performance while maintaining test coverage and confidence. Focus on:

1. **Combining tests** - Fewer tests with more expectations vs many tests with single expectations
2. **Reducing setup overhead** - Test setup is expensive, execution is cheap
3. **Using fastest test strategies** - Optimize object creation methods
4. **Prioritizing by test type speed** - System (slowest) → Request → Unit (fastest)

### Step 1: Identify Changed Test Files
```bash
git diff --name-only main...HEAD | grep -E "spec/.*_spec\.rb$"
```

### Step 2: Review Tests by Type (Slowest to Fastest)

**Note**: During development, tests may have been added in verbose or debugging-focused ways to understand behavior. Look for tests that seem to be for debugging purposes rather than actual coverage - these should be **removed entirely**.

#### A. System Tests (Slowest - Highest Impact)
**Philosophy**: System tests should focus ONLY on happy path user journeys unless significant product behavior changes were made.

**Review Criteria**:
- [ ] Are there multiple system tests that could be combined into a single user journey?
- [ ] Are we testing edge cases that belong in request specs?
- [ ] Are we testing simple interactions (like Cancel buttons) that don't need full browser simulation?
- [ ] Can we remove duplicate coverage between system and request specs?
- [ ] Are there debugging-focused tests that can be removed entirely?

**Optimization Actions**:
- Combine related user flows into single comprehensive tests
- Move validation errors and edge cases to request specs
- Remove system tests for simple UI interactions
- Keep only 1-2 system tests per major feature unless significant product changes

#### B. Request Tests (Medium Speed)
**Philosophy**: Group related API behaviors in single tests. Test setup is expensive, execution is cheap.

**Review Criteria**:
- [ ] Can we combine multiple related API calls in a single test?
- [ ] Are we testing multiple scenarios that share expensive setup?
- [ ] Can we test create → update → delete flows in sequence?
- [ ] Are we duplicating coverage with system tests?
- [ ] Are there debugging-focused tests exploring edge cases that can be removed?

**Optimization Actions**:
- Combine related CRUD operations in single tests
- Group authentication/authorization scenarios
- Use sequential API calls within same test when testing related functionality
- Example: `it 'creates item, updates it, and deletes it successfully'`

#### C. Unit Tests (Fastest - Lower Impact but Still Important)
**Philosophy**: Use fastest object creation strategy that doesn't compromise the test's purpose.

**Object Creation Strategy (fastest to slowest)**:
1. `ObjectName.new` or `ObjectName.new(attributes_for(:object_name))` (fastest)
2. `build_stubbed(:object_name)`
3. `build(:object_name)`
4. `create(:object_name)` (slowest - only when database needed)

**Review Criteria**:
- [ ] Are we using `create` when `build` or `build_stubbed` would work?
- [ ] Are we setting up complex object graphs when simple objects suffice?
- [ ] Can we combine multiple unit test assertions into fewer tests?
- [ ] Are we testing framework behavior instead of custom logic?
- [ ] Are there verbose debugging tests that were added to understand behavior but aren't needed for coverage?

**When to use `create`**:
- Testing database queries, scopes, or validations that require persistence
- Testing callbacks that require database state
- Complex associations that are hard to stub
- When stubbing becomes more complex than using real objects

**Optimization Actions**:
- Replace `create` with faster alternatives where possible
- Combine related unit test assertions
- Remove tests of framework functionality (validations, basic associations)
- Use `instance_double` for external dependencies
- **Remove debugging tests entirely** - Look for tests with names like "test behavior", "check if X works", or overly verbose exploratory tests

### Common Debugging Test Patterns to Remove

**Tests that should be removed entirely:**
- Tests with vague names like `"it works"`, `"test behavior"`, `"debugging issue"`
- Multiple tests exploring the same edge case with slight variations
- Tests that seem to verify framework behavior rather than application logic
- Overly verbose tests that print output or have excessive logging
- Tests that duplicate existing coverage but with different data
- Exploratory tests that were added to understand third-party library behavior

**Examples of debugging tests to remove:**
```ruby
# ❌ REMOVE - Debugging test
it "debugs the issue with customer creation" do
  puts "Testing customer creation..."
  customer = create(:customer)
  puts "Customer created: #{customer.id}"
  expect(customer).to be_persisted
end

# ❌ REMOVE - Exploring framework behavior
it "checks if ActiveRecord callbacks work as expected" do
  customer = build(:customer)
  expect { customer.save! }.to change { customer.created_at }.from(nil)
end

# ❌ REMOVE - Overly verbose exploration
it "tests various combinations of customer attributes" do
  # 20 lines of different attribute combinations that don't add value
end
```

### Step 3: Implementation Process

For each test file found:

1. **Analyze current structure**:
   ```bash
   # Count current tests and identify slow patterns
   grep -c "it '" path/to/spec_file.rb
   grep -c "create(" path/to/spec_file.rb
   ```

2. **Apply optimizations DIRECTLY to existing files**:
   - Use the Edit tool to modify the existing spec file in place
   - Combine related `it` blocks into fewer, more comprehensive tests
   - Replace object creation with faster alternatives
   - Remove redundant coverage
   - Consolidate setup code
   - **NEVER create new files** - always edit the original

3. **Verify coverage maintained**:
   - Ensure all original assertions are preserved in the modified file
   - Check that error cases are still covered
   - Confirm happy path scenarios remain intact

### Step 4: Validation

After optimization, verify:
- [ ] All original test assertions are preserved in combined tests
- [ ] Test names accurately reflect what's being tested
- [ ] No loss of edge case coverage (moved to appropriate test types)
- [ ] Faster object creation strategies are used appropriately

## Example Optimization

### Before (Multiple tests, slow setup):
```ruby
describe "POST /api/items" do
  it "creates item with valid attributes" do
    user = create(:user)
    post "/api/items", params: { name: "Test" }, headers: auth_headers(user)
    expect(response).to have_http_status(:created)
  end

  it "returns created item data" do
    user = create(:user)
    post "/api/items", params: { name: "Test" }, headers: auth_headers(user)
    expect(response.parsed_body["name"]).to eq("Test")
  end

  it "increments item count" do
    user = create(:user)
    expect {
      post "/api/items", params: { name: "Test" }, headers: auth_headers(user)
    }.to change(Item, :count).by(1)
  end
end
```

### After (Combined test, shared setup):
```ruby
describe "POST /api/items" do
  it "creates item, returns data, and increments count" do
    user = create(:user)

    expect {
      post "/api/items", params: { name: "Test" }, headers: auth_headers(user)
    }.to change(Item, :count).by(1)

    expect(response).to have_http_status(:created)
    expect(response.parsed_body["name"]).to eq("Test")
  end
end
```

## Success Metrics

- **Reduced test count** while maintaining coverage
- **Fewer `create` calls** in unit tests
- **System tests focus on happy path** only
- **Related behaviors tested together** in single tests
- **Faster test suite execution** overall

## Notes

- **Always edit existing files directly** - never create new files with different names
- Always run all the tests in this current branch after optimization to ensure nothing breaks
- Focus on files with the most changed lines for highest impact
- When in doubt, prefer slightly slower tests that are easier to understand
- Document any significant behavior combinations in test names
- The goal is to **replace** the original specs with optimized versions, not add to them
